{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recruit Restaurant Visitor Forecasting\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style='color: red'> Problem Statement: </h3>\n",
    "\n",
    "<p>\n",
    "    Running a thriving local restaurant isn't always as charming as first impressions appear. There are often all sorts of unexpected troubles popping up that could hurt business.\n",
    "\n",
    "<strong>One common predicament is that restaurants need to know how many customers to expect each day to effectively purchase ingredients and schedule staff members.</strong> This forecast isn't easy to make because many unpredictable factors affect restaurant attendance, like weather and local competition. It's even harder for newer restaurants with little historical data.\n",
    "\n",
    "Recruit Holdings has unique access to key datasets that could make automated future customer prediction possible. Specifically, Recruit Holdings owns Hot Pepper Gourmet (a restaurant review service), AirREGI (a restaurant point of sales service), and Restaurant Board (reservation log management software).\n",
    "\n",
    "<strong>In this competition, you're challenged to use reservation and visitation data to predict the total number of visitors to a restaurant for future dates. This information will help restaurants be much more efficient and allow them to focus on creating an enjoyable dining experience for their customers.</strong>\n",
    "</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<h3 style='color:red'> Loss function: RMSLE </h3>\n",
    "\n",
    "<p>\n",
    "RMSLE has this unique feature of penalizing underprediction compared to overprediction, which is important in this problem since we don't want the restaurants to be underprepared especially in case of small restaurants. Being over prepared has less effects on the business as the extra resources can be stored.\n",
    "</p>\n",
    "\n",
    "few other advantages:\n",
    "\n",
    "    * RMSLE is unaffected by the outlier values while RMSE does\n",
    "    * RMSLE focuses on the relational ratio difference between prediction and actual values while RMSE focuses on the difference between their magnitudes.\n",
    "\n",
    "[read more](https://medium.com/analytics-vidhya/root-mean-square-log-error-rmse-vs-rmlse-935c6cc1802a)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description(Kaggle)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-27T14:58:48.322321Z",
     "start_time": "2020-10-27T14:58:48.306361Z"
    }
   },
   "source": [
    "\n",
    "1. **air_visit_data.csv**: This data contains the historical visits done to AIR registered restaurants\n",
    "\n",
    "    * **air_store_id** : Unique ID for AIR registered restaurants\n",
    "    * **visit_date**   : The date of the day\n",
    "    * **visitors**     : No. of customers visited the restaurant\n",
    "    \n",
    "    \n",
    "2. **air_reserve.csv**: This data contains the reservation done using AIR reservation system\n",
    "    \n",
    "    * **air_store_id**       : Unique ID for AIR registered restaurants\n",
    "    * **visit_datetime**     : The visiting date done through reservation\n",
    "    * **reserve_datetime**   : The date on which reservation was made.\n",
    "    * **reserve_visitors**   : No. of visitors for the reservation\n",
    "    \n",
    "    \n",
    "3. **air_store_data**: This data contains the location and restaurant type information for AIR\n",
    "\n",
    "    * **air_store_id**       : Unique ID for AIR registered restaurants\n",
    "    * **air_genre_name**     : Type of restaurant\n",
    "    * **air_area_name**      : area name of the restaurant\n",
    "    * **latitude**           : lat of the restaurant\n",
    "    * **longitude**          : long of the restaurant\n",
    "\n",
    "\n",
    "4. **date_info.csv**: This data contains the visting day calendar information\n",
    "\n",
    "    * **calendar_date**      : The date\n",
    "    * **day_of_week**        : what day\n",
    "    * **holiday_flg**        : if the day was holiday? 1:yes; 0:No\n",
    "\n",
    "\n",
    "5. **hpg_reserve.csv**: This data contains the reservations done through HPG reservation system\n",
    "\n",
    "    * **hpg_store_id**       : Unique ID for the restaurants in HPG database\n",
    "    * **visit_datetime**     : the time of the reservation\n",
    "    * **reserve_datetime**   : the time the reservation was made\n",
    "    * **reserve_visitors**   : the number of visitors for that reservation\n",
    "\n",
    "\n",
    "6. **hpg_store_info.csv**: This data contains the location and restaurant type information for HPG\n",
    "\n",
    "    * **hpg_store_id**       : Unique ID for HPG registered restaurants\n",
    "    * **hpg_genre_name**     : Type of restaurant\n",
    "    * **hpg_area_name**      : area name of the restaurant\n",
    "    * **latitude**           : lat of the restaurant\n",
    "    * **longitude**          : long of the restaurant\n",
    "\n",
    "\n",
    "7. **store_id_relation.csv**: This data contains the mapping for HPG restaurants ID to AIR restaurant ID\n",
    "\n",
    "    * **hpg_store_id**       : HPG unique ID\n",
    "    * **air_store_id**       : AIR unique ID\n",
    "\n",
    "\n",
    "8. **sample_submission.csv**: This is test data for which the predictions has to be done.\n",
    "    \n",
    "    * **id**                 : The id is formed by concatenating the air_store_id and visit_date with an underscore\n",
    "    * **visitors**           : The number of visitors forecasted for the store and date combination\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T04:51:35.048738Z",
     "start_time": "2020-10-28T04:51:34.952996Z"
    },
    "code_folding": [
     19,
     27,
     70,
     302,
     328,
     358,
     371,
     401
    ]
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.feature_selection import RFECV\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import make_scorer\n",
    "import xgboost as xgb\n",
    "import unidecode\n",
    "\n",
    "data_path = os.path.join(os.path.curdir, \"Kaggle_Data\")\n",
    "processed_data_path = os.path.join(os.path.curdir, \"Processed_Data\")\n",
    "results_path = os.path.join(os.path.curdir, 'results')\n",
    "\n",
    "class RRVF():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.data = self.prepare_process_data()\n",
    "        self.features = self.feature_selection_RFE(self.data)\n",
    "        \n",
    "        # load the xgboost model\n",
    "        self.xgb_model = pickle.load(open(results_path + \"/xgb_regression_0.47827.pickle\", 'rb'))\n",
    "        \n",
    "    def get_data(self, train_only=False, test_only=False, both=False, feature_selected=False):\n",
    "        \"\"\"\n",
    "        This function will fetch the required data from the whole processed data\n",
    "        \"\"\"\n",
    "        if not feature_selected:\n",
    "            if train_only:\n",
    "                print(\"fetching full train data...done\")\n",
    "                return self.data.loc[self.data.is_train == True, :] \n",
    "            elif test_only:\n",
    "                print(\"fetching full test data...done\")\n",
    "                return self.data.loc[self.data.is_train == False, :]\n",
    "            else:\n",
    "                print(\"fetching full train+test data...done\")\n",
    "                return self.data\n",
    "        else:\n",
    "            \n",
    "            if train_only:\n",
    "                print(\"fetching feature_selected train data...done\")\n",
    "                temp = self.data.loc[self.data.is_train == True, self.features] \n",
    "                temp = pd.get_dummies(temp, columns=['day_of_week', \n",
    "                                             'area_prefecture', \n",
    "                                             'area_sub_prefecture', \n",
    "                                             'air_genre_name'])\n",
    "                return temp\n",
    "            \n",
    "            elif test_only:\n",
    "                print(\"fetching feature_selected test data...done\")\n",
    "                temp = self.data.loc[self.data.is_train == False, self.features]\n",
    "                temp = pd.get_dummies(temp, columns=['day_of_week', \n",
    "                                             'area_prefecture', \n",
    "                                             'area_sub_prefecture', \n",
    "                                             'air_genre_name'])\n",
    "                return temp\n",
    "            else:\n",
    "                print(\"fetching feature_selected train+test data...done\")\n",
    "                temp = self.data.loc[:, self.features]\n",
    "                temp = pd.get_dummies(temp, columns=['day_of_week', \n",
    "                                             'area_prefecture', \n",
    "                                             'area_sub_prefecture', \n",
    "                                             'air_genre_name'])\n",
    "                return temp\n",
    "            \n",
    "            \n",
    "    def prepare_process_data(self):\n",
    "        \"\"\"\n",
    "        This function will read multiple data sources, preprocess, featurize and return the data\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # read the required data:\n",
    "        # air\n",
    "        air_visit_data = pd.read_csv(data_path + \"/air_visit_data.csv\")\n",
    "        air_visit_data.visit_date = pd.to_datetime(air_visit_data.visit_date)\n",
    "        air_reservation_data = pd.read_csv(data_path + \"/air_reserve.csv\")\n",
    "        air_store_info = pd.read_csv(data_path + \"/air_store_info.csv\")\n",
    "\n",
    "        # date info\n",
    "        date_info = pd.read_csv(data_path + \"/date_info.csv\")\n",
    "        date_info.calendar_date = pd.to_datetime(date_info.calendar_date)\n",
    "\n",
    "        # hpg\n",
    "        hpg_reservation_data = pd.read_csv(data_path + \"/hpg_reserve.csv\")\n",
    "        store_id_relation = pd.read_csv(data_path + \"/store_id_relation.csv\")\n",
    "\n",
    "        # submission\n",
    "        submission = pd.read_csv(data_path + \"/sample_submission.csv\")\n",
    "\n",
    "        # weather information\n",
    "        air_store_weather_info = pd.read_csv(data_path + \"/processed_air_store_weather_info.csv\")\n",
    "        \n",
    "        print(\"Reading data from multiple sources...Done \\n\")\n",
    "\n",
    "\n",
    "        # prepare the test data\n",
    "        submission['air_store_id'] = submission.id.apply(lambda x: \"_\".join(x.split(\"_\")[:2]))\n",
    "        submission['visit_date'] = submission.id.apply(lambda x: x.split(\"_\")[2])\n",
    "        submission.drop(\"id\", axis=1, inplace=True)\n",
    "        submission = submission[['air_store_id', 'visit_date', 'visitors']]\n",
    "        submission.visit_date = pd.to_datetime(submission.visit_date)\n",
    "        \n",
    "        print(\"Test data prepared...Done\")\n",
    "\n",
    "        # add train and test tags\n",
    "        air_visit_data['is_train'] = True\n",
    "        submission['is_train'] = False\n",
    "\n",
    "        # combine the train and test data into one\n",
    "        data = pd.concat([air_visit_data, submission])\n",
    "\n",
    "        # join the train and test data together\n",
    "        data = pd.merge(data, air_store_info, how='left', on='air_store_id')\n",
    "        \n",
    "        \n",
    "        # add the date information from the date_info data\n",
    "        date_info['non_working'] = np.where(date_info.day_of_week.isin(['Saturday', 'Sunday']) | (date_info.holiday_flg == 1), 1, 0)\n",
    "        date_info['prev_day_holiday'] = date_info['non_working'].shift().fillna(0)\n",
    "        date_info['next_day_holiday'] = date_info['non_working'].shift(-1).fillna(0)\n",
    "        date_info.rename(columns={'calendar_date': 'visit_date'}, inplace=True)\n",
    "        data = pd.merge(data, date_info, how='left', on='visit_date')\n",
    "        \n",
    "        print(\"Date information collection...Done\")\n",
    "    \n",
    "        \n",
    "        ## -------------weather-information-store------------------------\n",
    "        # add the weather information using air store weather information data\n",
    "        air_store_weather_info.rename(columns={'calendar_date': 'visit_date'}, inplace=True)\n",
    "        air_store_weather_info.visit_date = pd.to_datetime(air_store_weather_info.visit_date)\n",
    "        data = pd.merge(data, air_store_weather_info, how='left', on=['air_store_id', 'visit_date'])\n",
    "        ## -------------weather-information-store------------------------\n",
    "        \n",
    "        \n",
    "        print(\"Weather information collection...Done\")\n",
    "        \n",
    "        # handle outliers\n",
    "        data = self.handle_outliers(data)\n",
    "        \n",
    "        print(\"Outliers Detection and handling...Done\")\n",
    "        \n",
    "        \n",
    "        ## -------------restaurant-count-feature------------------------\n",
    "        # area wise restaurant count\n",
    "        area_wise_store_count = air_store_info[['air_store_id', 'air_area_name']].groupby('air_area_name').count()\n",
    "        area_wise_store_count.rename(columns={'air_store_id': 'area_store_count'}, inplace=True)\n",
    "        data = pd.merge(data, area_wise_store_count, how='left', on='air_area_name')\n",
    "        \n",
    "        # area wise genre count\n",
    "        area_wise_genre_count = air_store_info[['air_store_id', 'air_genre_name', 'air_area_name']].groupby(['air_area_name', 'air_genre_name']).count()\n",
    "        area_wise_genre_count.rename(columns={'air_store_id': 'area_genre_count'}, inplace=True)\n",
    "        data = pd.merge(data, area_wise_genre_count, how='left', on=['air_area_name', 'air_genre_name'])\n",
    "        ## -------------restaurant-count-feature------------------------\n",
    "        \n",
    "        print(\"Area wise resturant, genre's count ... Done\")\n",
    "        \n",
    "        ## ------------- reservation feature ----------------------------\n",
    "        # get the reservations done on HPG\n",
    "        hpg_reserve = pd.merge(store_id_relation, hpg_reservation_data, how='left', on='hpg_store_id')\n",
    "        hpg_reserve.drop(\"hpg_store_id\", axis=1, inplace=True)\n",
    "\n",
    "        # concat both AIR and HPG reservations\n",
    "        reservation_data = pd.concat([air_reservation_data, hpg_reserve])\n",
    "\n",
    "        # add the hour gap diff between reseravtion time as a feature\n",
    "        reservation_data.visit_datetime = pd.to_datetime(reservation_data.visit_datetime)\n",
    "        reservation_data.reserve_datetime = pd.to_datetime(reservation_data.reserve_datetime)\n",
    "        reservation_data['reservation_gap'] = reservation_data.visit_datetime - reservation_data.reserve_datetime\n",
    "        reservation_data['reservation_gap'] = reservation_data.reservation_gap / np.timedelta64(1,'h')\n",
    "\n",
    "\n",
    "        # encode the visitors based on the reservation gap \n",
    "        # 6th place solutuon features\n",
    "        # if reservation gap is under 12 hr\n",
    "        reservation_data['reserve_visitor_lt_12hr'] = np.where(reservation_data.reservation_gap < 12, reservation_data.reserve_visitors, 0)\n",
    "\n",
    "        # if reservation gap is between 12-36 hr\n",
    "        reservation_data['reserve_visitor_bt_12_36'] = np.where((reservation_data.reservation_gap >= 12) & (reservation_data.reservation_gap < 36), \n",
    "                                                                reservation_data.reserve_visitors, 0)\n",
    "\n",
    "        # if reservation gap is between 37-59 hr\n",
    "        reservation_data['reserve_visitor_bt_36_59'] = np.where((reservation_data.reservation_gap >= 36) & (reservation_data.reservation_gap < 59), \n",
    "                                                                reservation_data.reserve_visitors, 0)\n",
    "\n",
    "        # if reservation gap is between 59-85 hr\n",
    "        reservation_data['reserve_visitor_bt_59_85'] = np.where((reservation_data.reservation_gap >= 59) & (reservation_data.reservation_gap < 85), \n",
    "                                                                reservation_data.reserve_visitors, 0)\n",
    "\n",
    "        # if reservation gap is greater 85 hr\n",
    "        reservation_data['reserve_visitor_gt_85'] = np.where(reservation_data.reservation_gap >= 85, reservation_data.reserve_visitors, 0)\n",
    "        reservation_data['visit_date'] = reservation_data.visit_datetime.dt.date\n",
    "\n",
    "        # group by per store on visit date = reservation visitors\n",
    "        reservation_features = reservation_data.groupby(['air_store_id', 'visit_date'], as_index=False)[['reserve_visitors', 'reserve_visitor_lt_12hr', \n",
    "                                                                                                         'reserve_visitor_bt_12_36', 'reserve_visitor_bt_36_59',\n",
    "                                                                                                        'reserve_visitor_bt_59_85', 'reserve_visitor_gt_85']].sum()\n",
    "        # log transform the reservation featues\n",
    "        reservation_columns = reservation_features.columns[2:]\n",
    "        for column in reservation_columns:\n",
    "            reservation_features[column] = reservation_features[column].apply(lambda a: np.log1p(a))\n",
    "            \n",
    "        reservation_features.visit_date = pd.to_datetime(reservation_features.visit_date)\n",
    "        data = pd.merge(data, reservation_features, how='left', on=['air_store_id', 'visit_date']).fillna(0)\n",
    "        ## ------------- reservation feature --------------------------\n",
    "        \n",
    "        print(\"Reservation guest number ....Done\")\n",
    "        \n",
    "        ## --------------date features --------------------------------\n",
    "        data['visit_day'] = data.visit_date.dt.day\n",
    "        data['visit_month'] = data.visit_date.dt.month\n",
    "        data['visit_year'] = data.visit_date.dt.year\n",
    "        data['visit_week'] = data.visit_date.dt.weekofyear\n",
    "        ## --------------date features --------------------------------\n",
    "        \n",
    "        print(\"Data Features...Done\")\n",
    "        \n",
    "        ##---------------monthly-visitor-statistics----------------------\n",
    "        # calculating monthly visitors mean for each restaurants\n",
    "        month_wise_mean= data.groupby(['air_store_id','visit_month'],as_index=False)['visitors_capped'].mean()\n",
    "        month_wise_mean = month_wise_mean.pivot(index='air_store_id', columns='visit_month', values='visitors_capped').reset_index()\n",
    "        month_wise_mean.columns = ['air_store_id','m1','m2','m3','m4','m5','m6','m7','m8','m9','m10','m11','m12']\n",
    "\n",
    "        month_wise_mean_2= data.groupby(['air_store_id','visit_month'],as_index=False)['visitors_capped_log1p'].mean()\n",
    "        month_wise_mean_2 = month_wise_mean_2.pivot(index='air_store_id', columns='visit_month', values='visitors_capped_log1p').reset_index()\n",
    "        month_wise_mean_2.columns = ['air_store_id','m1_log1p','m2_log1p','m3_log1p','m4_log1p','m5_log1p','m6_log1p','m7_log1p','m8_log1p','m9_log1p',\n",
    "                                   'm10_log1p','m11_log1p','m12_log1p']\n",
    "        # merging with data\n",
    "        data = data.merge(month_wise_mean,on='air_store_id',how='left')\n",
    "        data = data.merge(month_wise_mean_2,on='air_store_id',how='left')\n",
    "        ##---------------monthly-visitor-statistics-------------------------\n",
    "        \n",
    "        print(\"Monthly visitors statistics... Done\")\n",
    "    \n",
    "        ## --------------visitors-statistics----------------------------------\n",
    "        store_non_working = self.visitor_statistics(data, group_by=['air_store_id', 'non_working'], on='visitors_capped')\n",
    "        store_dow = self.visitor_statistics(data, group_by=['air_store_id', 'day_of_week'], on='visitors_capped')\n",
    "        store = self.visitor_statistics(data, group_by=['air_store_id'], on='visitors_capped')\n",
    "        \n",
    "        store_non_working_1 = self.visitor_statistics(data, group_by=['air_store_id', 'non_working'], on='visitors_capped_log1p')\n",
    "        store_dow_1 = self.visitor_statistics(data, group_by=['air_store_id', 'day_of_week'], on='visitors_capped_log1p')\n",
    "        store_1 = self.visitor_statistics(data, group_by=['air_store_id'], on='visitors_capped_log1p')\n",
    "        \n",
    "        data = pd.merge(data, store_non_working, how='left', on=['air_store_id', 'non_working'])\n",
    "        data = pd.merge(data, store_dow, how='left', on=['air_store_id', 'day_of_week'])\n",
    "        data = pd.merge(data, store, how='left', on=['air_store_id'])\n",
    "    \n",
    "        data = pd.merge(data, store_non_working_1, how='left', on=['air_store_id', 'non_working'])\n",
    "        data = pd.merge(data, store_dow_1, how='left', on=['air_store_id', 'day_of_week'])\n",
    "        data = pd.merge(data, store_1, how='left', on=['air_store_id'])\n",
    "        ## --------------visitors-statistics----------------------------------\n",
    "        \n",
    "        print(\"Store wise, working, day of week visitors statistics...Done\")\n",
    "        \n",
    "        ##------------------prefecures-feature--------------------------------\n",
    "        data['area_prefecture'] = data.air_area_name.apply(lambda x: x.split()[0])\n",
    "        data['area_sub_prefecture'] = data.air_area_name.apply(lambda x: x.split()[1])\n",
    "        ##------------------prefecures-feature--------------------------------\n",
    "        \n",
    "        print(\"Area prefectures...Done\")\n",
    "        \n",
    "        ##------------------weekly_restaurants_count-----------------------------\n",
    "        weekly_count = self.weekly_restaurants_count(data)\n",
    "        data = pd.merge(data, weekly_count, how='left', on=['visit_year', 'visit_week'])\n",
    "        ##------------------weekly_restaurants_count-----------------------------\n",
    "        \n",
    "        print(\"Weekly resturant open..Done\")\n",
    "        \n",
    "        ##------------------nb-reservation-visit-date----------------------------\n",
    "        reservation_count_feature = reservation_data.groupby(['air_store_id', 'visit_date'], as_index=False).visit_datetime.count()\n",
    "        reservation_count_feature.visit_date = pd.to_datetime(reservation_count_feature.visit_date)\n",
    "        reservation_count_feature.rename(columns={'visit_datetime' : 'reservation_count'}, inplace=True)\n",
    "        data = pd.merge(data, reservation_count_feature, how='left', on=['air_store_id', 'visit_date']).fillna(0)\n",
    "        data.reservation_count = data.reservation_count.apply(lambda x: np.log1p(x))\n",
    "        ##------------------nb-reservation-visit-date----------------------------\n",
    "        \n",
    "        print(\"Reservations...Done\")\n",
    "        \n",
    "        ##-------------------random-features--------------------------------------\n",
    "        data['lat_plus_long'] = data.latitude + data.longitude\n",
    "        data['lat_max_diff'] = data.latitude.max() - data.latitude \n",
    "        data['long_max_diff'] = data.longitude.max() - data.longitude \n",
    "        ##-------------------random-features--------------------------------------\n",
    "        \n",
    "        print(\"Random features...Done\\n\")\n",
    "        \n",
    "        # fix accented characters\n",
    "        data['area_prefecture'] = data['area_prefecture'].apply(lambda x: unidecode.unidecode(x))\n",
    "        data['area_sub_prefecture'] = data['area_sub_prefecture'].apply(lambda x: unidecode.unidecode(x))\n",
    "        \n",
    "        # set air_id+ date as index\n",
    "        data.sort_values(['air_store_id', 'visit_date'], inplace=True)\n",
    "        data['id'] = data.air_store_id + \"_\" + data.visit_date.astype('str')\n",
    "        data.set_index('id', inplace=True)\n",
    "        data.drop(['air_store_id', 'visit_date'], axis=1, inplace=True)\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "    def weekly_restaurants_count(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        This function will return the count of restaurants that are open on the given week\n",
    "        \"\"\"\n",
    "        total_data = df.copy()\n",
    "        total_data.loc[total_data.visit_week == 53, 'visit_week'] = 0\n",
    "        week_list = list(total_data['visit_week'].unique())\n",
    "        year_list = list(total_data['visit_year'].unique())\n",
    "\n",
    "        year_week_count = []\n",
    "        for year in year_list:\n",
    "            for week_num in week_list:\n",
    "\n",
    "                count = len(list(total_data.loc[(total_data.visit_year ==year) & \n",
    "                                                (total_data.visit_week == week_num),'air_store_id'].unique()))\n",
    "                 # upto may 2017\n",
    "                if (year == 2017) and (week_num>22):\n",
    "                    break\n",
    "\n",
    "                year_week_count.append([year, week_num, count])\n",
    "\n",
    "        columns = ['visit_year', 'visit_week', 'open_resturant_count']\n",
    "        #restaurant_weekly_open = pd.DataFrame(year_week_count, columns=columns)\n",
    "        return pd.DataFrame(year_week_count, columns=columns)\n",
    "        \n",
    "        \n",
    "    def visitor_statistics(self, df, group_by, on):\n",
    "        \"\"\"\n",
    "        This function will add visior statistics, by day, store, and non working day\n",
    "        \"\"\"\n",
    "        \n",
    "        temp = df.groupby(group_by, as_index=False)\n",
    "        \n",
    "        # mean\n",
    "        stats = temp[on].mean()\n",
    "        stats.rename(columns={on: f'mean_{on}_{\"_\".join(group_by)}'}, inplace=True)\n",
    "        \n",
    "        # median\n",
    "        stats = pd.merge(stats, temp[on].median(), how='left', on=group_by)\n",
    "        stats.rename(columns={on: f'median_{on}_{\"_\".join(group_by)}'}, inplace=True)\n",
    "        \n",
    "        # minimum\n",
    "        stats = pd.merge(stats, temp[on].min(), how='left', on=group_by)\n",
    "        stats.rename(columns={on: f'min_{on}_{\"_\".join(group_by)}'}, inplace=True)\n",
    "        \n",
    "        # maximum\n",
    "        stats = pd.merge(stats, temp[on].max(), how='left', on=group_by)\n",
    "        stats.rename(columns={on: f'max_{on}_{\"_\".join(group_by)}'}, inplace=True)\n",
    "        \n",
    "        # count\n",
    "        stats = pd.merge(stats, temp[on].count(), how='left', on=group_by)\n",
    "        stats.rename(columns={on: f'count_{on}_{\"_\".join(group_by)}'}, inplace=True)\n",
    "        \n",
    "        return stats\n",
    "        \n",
    "        \n",
    "    def calulate_IQR_outlier_range(self, df):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function will calculate the higher max IQR value for given values\n",
    "        \"\"\"\n",
    "\n",
    "        q1, q3 = np.quantile(df.values, [0.25, 0.75])\n",
    "        higher_IQR = q3 + (1.5 * (q3 - q1))\n",
    "        \n",
    "        # return the minimum\n",
    "        return min(higher_IQR, df.max())\n",
    "\n",
    "    \n",
    "    def handle_outliers(self, df: pd.DataFrame):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function will handle the outliers, by capping them to their max outlier range\n",
    "        \"\"\"\n",
    "        \n",
    "        # group the data by stores\n",
    "        stores = df[['air_store_id', 'visitors']].groupby('air_store_id')\n",
    "        \n",
    "        # calculate the higher IQR range for each store(No negative visitors)\n",
    "        max_store_vistors_capped = stores.apply(lambda x: self.calulate_IQR_outlier_range(x.visitors))\n",
    "        max_store_vistors_capped.name = 'max_visitor'\n",
    "        \n",
    "        # add the max_capped_data for each store\n",
    "        df = pd.merge(df, max_store_vistors_capped, how='left', on='air_store_id')\n",
    "        \n",
    "        # minimum of max data and the original \n",
    "        df['visitors_capped'] = np.where(df.visitors < df.max_visitor, \n",
    "                                                df.visitors, df.max_visitor)\n",
    "        \n",
    "        # add the log transformation of the visitors too\n",
    "        df['visitors_log1p'] = df.visitors.apply(lambda x: np.log1p(x))\n",
    "        df['visitors_capped_log1p'] = df.visitors_capped.apply(lambda x: np.log1p(x))\n",
    "        \n",
    "        # drop the max visitor\n",
    "        df.drop('max_visitor', axis=1, inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def rmsle(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        This function will calculate the RMSLE score given the predicition and true values\n",
    "        \"\"\"\n",
    "\n",
    "        # get exp of log predictions\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_true = np.expm1(y_true)\n",
    "\n",
    "        # calculate the rmsle\n",
    "        # formula \n",
    "        return np.sqrt(np.mean(np.square(np.log1p(y_true) - np.log1p(y_pred))))\n",
    "    \n",
    "    \n",
    "    def feature_selection_RFE(self, df):\n",
    "        \"\"\"\n",
    "        This function will selected the best features using RFE method, and return the list of column names\n",
    "        \"\"\"\n",
    "        \n",
    "        # feature with rank 1\n",
    "        selected_features = list(pd.read_csv(processed_data_path + '/feature_eliminated_train.csv').columns)\n",
    "        \n",
    "        print(\"Feature selection process running...Done\")\n",
    "        print(\"No of selected features: \", len(selected_features))\n",
    "        \n",
    "        return selected_features\n",
    "        \n",
    "    # train\n",
    "    def function1(self, train_data, train_target):\n",
    "        \n",
    "        \"\"\"\n",
    "        This function will take training data, training targets and calculate the RMSLE Score\n",
    "        \"\"\"\n",
    "        \n",
    "        # convert to xgb matrix \n",
    "        x_train_matrix = xgb.DMatrix(data=train_data, label=train_target)\n",
    "        \n",
    "        # predict the train matrix outputs\n",
    "        xgb_train_predictions = self.xgb_model.predict(x_train_matrix)\n",
    "        \n",
    "        train_rmsle_score = self.rmsle(train_target.values, xgb_train_predictions)\n",
    "        \n",
    "        return train_rmsle_score\n",
    "        \n",
    "    # prediction\n",
    "    def function2(self, test_data):\n",
    "        \"\"\"\n",
    "        this function will take a array/single of queries and return the prediction\n",
    "        \"\"\"\n",
    "        \n",
    "        if type(test_data) == pd.Series:\n",
    "            id_values = test_data.values\n",
    "    \n",
    "        elif type(test_data) == str:\n",
    "            id_values = [test_data]\n",
    "            \n",
    "        elif type(test_data) == np.ndarray:\n",
    "            id_values = test_data\n",
    "        else:\n",
    "            return \"Please send the queries in Series/numpy/Single string format\"\n",
    "        \n",
    "        \n",
    "        # fetch the featurized vector for the test data\n",
    "        featurized_test_data = self.data.loc[:, self.features]\n",
    "        featurized_test_data = pd.get_dummies(featurized_test_data, columns=['day_of_week', \n",
    "                                                                             'area_prefecture', \n",
    "                                                                             'area_sub_prefecture', \n",
    "                                                                             'air_genre_name'])\n",
    "        \n",
    "        # get the processed data for query points\n",
    "        featurized_test_data = featurized_test_data.loc[featurized_test_data.index.isin(id_values), :]\n",
    "        \n",
    "        # convert it to Dmatrix for prediction\n",
    "        x_test_matrix = xgb.DMatrix(data=featurized_test_data.drop('visitors_capped_log1p', axis=1))\n",
    "        \n",
    "        return np.expm1(self.xgb_model.predict(x_test_matrix))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T04:51:58.685525Z",
     "start_time": "2020-10-28T04:51:35.051731Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from multiple sources...Done \n",
      "\n",
      "Test data prepared...Done\n",
      "Date information collection...Done\n",
      "Weather information collection...Done\n",
      "Outliers Detection and handling...Done\n",
      "Area wise resturant, genre's count ... Done\n",
      "Reservation guest number ....Done\n",
      "Data Features...Done\n",
      "Monthly visitors statistics... Done\n",
      "Store wise, working, day of week visitors statistics...Done\n",
      "Area prefectures...Done\n",
      "Weekly resturant open..Done\n",
      "Reservations...Done\n",
      "Random features...Done\n",
      "\n",
      "Feature selection process running...Done\n",
      "No of selected features:  66\n"
     ]
    }
   ],
   "source": [
    "# intantiate the class, this will prepare, process, feature select the data\n",
    "rv = RRVF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T04:53:08.242028Z",
     "start_time": "2020-10-28T04:51:58.687521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching feature_selected train data...done\n",
      "\n",
      "The RMSLE score on train data and its target:  0.4131801507919498\n",
      "\n",
      "Time taken to predict train_data(252108): 68.60 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# get the processed train data\n",
    "train =rv.get_data(train_only=True, feature_selected=True)\n",
    "\n",
    "start_time = time.time()\n",
    "# use function1 to get the RMSLE score for the train data\n",
    "score = rv.function1(train.drop('visitors_capped_log1p', axis=1), train['visitors_capped_log1p'])\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"\\nThe RMSLE score on train data and its target: \", score)\n",
    "print(f\"\\nTime taken to predict train_data({train.shape[0]}): {(end_time - start_time):.2f} secs\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T04:53:09.406913Z",
     "start_time": "2020-10-28T04:53:08.249011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict test_data: 1.07 secs\n",
      "prediction: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>visitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-24</td>\n",
       "      <td>21.351002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             query   visitors\n",
       "0  air_00a91d42b08b08d9_2017-04-24  21.351002"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the submision data to get predictions\n",
    "sub = pd.read_csv(data_path + \"/sample_submission.csv\")\n",
    "\n",
    "# send subset of queries in single str datapoint\n",
    "test = sub.iloc[1, 0]\n",
    "\n",
    "start_time = time.time()\n",
    "predicition = rv.function2(test)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken to predict test_data: {(end_time - start_time):.2f} secs\" )\n",
    "print(\"prediction: \")\n",
    "result = pd.DataFrame(dict(query=[test], visitors=predicition))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T04:53:10.405244Z",
     "start_time": "2020-10-28T04:53:09.408909Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict test_data: 0.93 secs\n",
      "prediction: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>visitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-23</td>\n",
       "      <td>1.817007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-24</td>\n",
       "      <td>21.351002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-25</td>\n",
       "      <td>25.375101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-26</td>\n",
       "      <td>29.888779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-27</td>\n",
       "      <td>30.829861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             query   visitors\n",
       "0  air_00a91d42b08b08d9_2017-04-23   1.817007\n",
       "1  air_00a91d42b08b08d9_2017-04-24  21.351002\n",
       "2  air_00a91d42b08b08d9_2017-04-25  25.375101\n",
       "3  air_00a91d42b08b08d9_2017-04-26  29.888779\n",
       "4  air_00a91d42b08b08d9_2017-04-27  30.829861"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the submision data to get predictions\n",
    "sub = pd.read_csv(data_path + \"/sample_submission.csv\")\n",
    "\n",
    "# send subset of queries in Series format\n",
    "test = sub.iloc[:5, 0]\n",
    "\n",
    "start_time = time.time()\n",
    "predicition = rv.function2(test)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken to predict test_data: {(end_time - start_time):.2f} secs\" )\n",
    "print(\"prediction: \")\n",
    "result = pd.DataFrame(dict(query=test.values, visitors=predicition))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T04:53:11.421525Z",
     "start_time": "2020-10-28T04:53:10.407238Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict test_data: 0.96 secs\n",
      "prediction: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>visitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-24</td>\n",
       "      <td>21.351002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             query   visitors\n",
       "0  air_00a91d42b08b08d9_2017-04-24  21.351002"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the submision data to get predictions\n",
    "sub = pd.read_csv(data_path + \"/sample_submission.csv\")\n",
    "\n",
    "# send subset of queries in single array datapoint\n",
    "test = sub.iloc[1, 0]\n",
    "\n",
    "start_time = time.time()\n",
    "predicition = rv.function2(np.array([test]))\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken to predict test_data: {(end_time - start_time):.2f} secs\" )\n",
    "print(\"prediction: \")\n",
    "result = pd.DataFrame(dict(query=[test], visitors=predicition))\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T04:53:12.456757Z",
     "start_time": "2020-10-28T04:53:11.423521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to predict test_data: 0.98 secs\n",
      "prediction: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>visitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-23</td>\n",
       "      <td>1.817007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-24</td>\n",
       "      <td>21.351002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-25</td>\n",
       "      <td>25.375101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-26</td>\n",
       "      <td>29.888779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_00a91d42b08b08d9_2017-04-27</td>\n",
       "      <td>30.829861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             query   visitors\n",
       "0  air_00a91d42b08b08d9_2017-04-23   1.817007\n",
       "1  air_00a91d42b08b08d9_2017-04-24  21.351002\n",
       "2  air_00a91d42b08b08d9_2017-04-25  25.375101\n",
       "3  air_00a91d42b08b08d9_2017-04-26  29.888779\n",
       "4  air_00a91d42b08b08d9_2017-04-27  30.829861"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the submision data to get predictions\n",
    "sub = pd.read_csv(data_path + \"/sample_submission.csv\")\n",
    "\n",
    "# send subset of queries in single array datapoint\n",
    "test = sub.iloc[:5, 0]\n",
    "\n",
    "start_time = time.time()\n",
    "predicition = rv.function2(test.values)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Time taken to predict test_data: {(end_time - start_time):.2f} secs\" )\n",
    "print(\"prediction: \")\n",
    "result = pd.DataFrame(dict(query=test.values, visitors=predicition))\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
